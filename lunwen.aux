\relax 
\providecommand\babel@aux[2]{}
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{diao2022metaformer}
\citation{sandler2018mobilenetv2}
\bibstyle{unsrt}
\bibdata{lunwen}
\bibcite{miyaguchi2022motif}{1}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}背景介绍}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}研究现状}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}原理和方案}{1}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}关键技术}{1}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}模型}{1}{subsection.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}vision-Transformer}{1}{subsubsection.4.1.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}MetaFormer}{1}{subsubsection.4.1.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces 该模型采用卷积层来提取视觉特征，然后通过Patch Embedding将图像特征转化为视觉标记。Code、Endemic和Binomial name的元信息被热编码，并通过Non-Liner Embedding获得元标记。视觉令牌、元令牌和类令牌使用相对转换层进行融合。融合后的标记在下面的注意区块中被连续聚合。最终输出的类标记用于类别预测。}}{1}{figure.1}\protected@file@percent }
\newlabel{figure 2}{{1}{1}{该模型采用卷积层来提取视觉特征，然后通过Patch Embedding将图像特征转化为视觉标记。Code、Endemic和Binomial name的元信息被热编码，并通过Non-Liner Embedding获得元标记。视觉令牌、元令牌和类令牌使用相对转换层进行融合。融合后的标记在下面的注意区块中被连续聚合。最终输出的类标记用于类别预测。}{figure.1}{}}
\bibcite{cynthia2023effective}{2}
\bibcite{wang2016multi}{3}
\bibcite{progga2021cnn}{4}
\bibcite{liu2023learn}{5}
\bibcite{hou2019multilayer}{6}
\bibcite{han2019p}{7}
\bibcite{fu2017look}{8}
\bibcite{fayou2022combining}{9}
\bibcite{amir2017image}{10}
\bibcite{yang2021snake}{11}
\bibcite{patel2020revealing}{12}
\bibcite{zhai2013heterogeneous}{13}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}损失函数}{2}{subsection.4.2}\protected@file@percent }
\bibcite{chu2019geo}{14}
\bibcite{he2017fine}{15}
\bibcite{bloch2022combination}{16}
\bibcite{bolon2022artificial}{17}
\bibcite{dosovitskiy2020image}{18}
\bibcite{diao2022metaformer}{19}
\bibcite{sandler2018mobilenetv2}{20}
\bibcite{zhang2018generalized}{21}
\bibcite{deng2019arcface}{22}
\bibcite{liu2022deep}{23}
\bibcite{oord2018representation}{24}
\gdef \@abspage@last{3}
